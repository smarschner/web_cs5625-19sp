<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><strong>Due: Tuesday Mar 5 2019 (11:59pm).</strong> Work on your own or in groups of 2, as you prefer. This assignment has a written task and 5 programming tasks.</p>

<h2>Written Task</h2>

<p>In this written assignment, we will deal with tangent vectors and shading frames. We’ve seen in class that we can compute tangent frames from the texture coordinates.  To recap: the basic idea is that we can define a linear mapping from 2D texture coordinates to 3D coordinates:  \(f(u, v) = (x, y, z)\). The derivative of this mapping gives us tangent vectors in the \(u\) and \(v\) directions per triangle. Then we can average the triangles' tangent vectors to get vertex tangents, in the same way as we do for normal vectors.</p>

<p>Assume we have a mesh drawn in the left image. It contains 6 triangles: \(\Delta X_1X_2X_7, \Delta X_2X_3X_7\), \(\Delta X_3X_4X_7\), \(\Delta X_4X_5X_7\), \(\Delta X_5X_6X_7\), and \(\Delta X_6X_1X_7\). The vertices’ 3D positions are:
    \[\begin{aligned}
    X_1 &= (0, 1, 0), \\
    X_2 &= (-\frac{\sqrt3}{2}, \frac{1}{2}, 0),\\
    X_3 &= (-\frac{\sqrt3}{2}, -\frac{1}{2}, 0),\\
    X_4 &= (0, -1, 0),\\
    X_5 &= (\frac{\sqrt3}{2}, -\frac{1}{2}, 0),\\
    X_6 &= (\frac{\sqrt3}{2}, \frac{1}{2}, 0),\\
    X_7 &= (0, 0, 1).\\
    \end{aligned}\]
</p>

<div class="thumbnails" style="text-align: center">
    <img src="images/a2/left.png" alt="3D coordinates">
    <img src="images/a2/right.png" alt="Texture coordinates">
</div>

<p>The image on the right gives the corresponding texture coordinates.<!--  Note that for all these triangles, the normal to the triangle is a vector from the origin to the center of that triangle. --> </p>

<p>a)    Compute the normal for each triangle.</p>
<p>b)    Compute the tangent vectors for each triangle. </p>
<p>c)    For each triangle, check whether the tangents are orthogonal to the normals and whether they are orthogonal to each other. Which orthogonalities are guaranteed to be true and why? Under what circumstances the normals will be orthogonal to the tangents?</p>
<p>d)    Average normal vectors and tangent vectors to get vertex normals and tangents.</p>
<p>e)    Compute the orthonormal frames at \(X_1\), \(X_2\), and \(X_7\). </p>
<p>f)    Calculate interpolated normals and tangents at the center of \(\Delta X_1X_2X_7\).</p>
<p>g)    Is the frame calculated in f) guaranteed to be orthonormal? Why? Compute the orthonormal frame if it is needed.</p>
<p>h)    If we change \(U_6\) to \((0.5, 0.7)\), can we still calculate tangent frames for each triangle? If not, what issue are we running into?  In practice, how might you make your code robust to this problem?</p>

You can pick your favorite language to do the calculations. Please submit a file with your answer to c), e), f), g) and h).</p>


      <h2>Programming Tasks</h2>

      <p>In this programming assignment, you will implement two different approaches to computing shaded images: forward rendering and deferred shading.  Within these two frameworks, you'll implement a set of shading models: Microfacet shading in both isotropic and anisotropic variants, and normal mapping to add lightweight surface relief.  In a separate program, you'll implement a post-processing operation that simulates bloom from bright image features, including directly viewed light sources and bright reflections.</p>


      <h3>Programming Task 1: Isotropic Microfacet Shading Model</h3>

      <p>The framework comes with a basic forward rendering path implemented that supports two materials: <tt>IsotropicMicrofacet</tt> and <tt>AnisotropicMicrofacet</tt>.  However, the shader implementations of all these materials only implement constant-color shading.</p>

      <p>To enable isotropic microfacet shading, edit <tt>resources/Shading/shaders/forward/isotropic_microfacet.frag</tt>
        to implement the microfacet reflectance model described in the paper <a href="http://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.html">"Microfacet Models for Refraction through Rough Surfaces"</a> by Walter et al., using the GGX model for the normal distribution function (NDF) that was introduced (to graphics) by that paper.  (Despite the name, the paper talks about models for both refraction and reflection.  However, we will only use the reflection part.)
      </p>

      <p>The parameters of the model are strored in the <tt>IsotropicMicrofacet</tt> class, and they are:
        <ul>
          <li>The diffuse color \(k_d\) in RGB format;</li>
          <li>The index of refraction \(\eta\), a scalar;</li>
          <li>The roughness parameter \(\alpha\), a scalar.</li>
        </ul>
        The class also maps these parameters to uniforms for the fragment shader, so you should not need to modify it at all.
      </p>

      <p>The solution implemented the model as follows. The RGB color of the fragment due to a light source is given by:
        $$\mathrm{color} = \bigg( k_d  + \frac{ F(\mathrm{i}, \mathrm{m}) D(\mathrm{m}) G(\mathrm{i}, \mathrm{o}, \mathrm{m})}{4 | \mathrm{i} \cdot \mathrm{n} | |\mathrm{o} \cdot \mathrm{n} | } \bigg) \max( 0, \mathrm{n} \cdot \mathrm{i} ) \times I$$
        where
        <ul>
          <li>\(\mathrm{n}\) is the normal vector at the point being shaded,</li>
          <li>\(\mathrm{i}\) is the unit vector from the shaded point to the light source,</li>
          <li>\(\mathrm{o}\) is the unit vector from the shaded point to the camera,</li>
          <li>\(\mathrm{m}\) is half vector between \(\mathrm{i}\) and \(\mathrm{o}\), in other words,
            $$ \mathrm{m} = \frac{\mathrm{i} + \mathrm{o}}{\| \mathrm{i} + \mathrm{o} \|},$$
          </li>
          <li>\(F(\mathrm{i}, \mathrm{m})\) is the Fresnel factor:
            $$ F(\mathrm{i}, \mathrm{m}) = \frac{1}{2} \frac{(g-c)^2}{(g+c)^2} \bigg( 1 + \frac{(c(g+c)-1)^2}{(c(g-c)+1)^2} \bigg) $$
            where \(g = \sqrt{\eta^2 - 1 + c^2}\) and \(c = |\mathrm{i} \cdot \mathrm{m}|\),
          </li>
          <li>\(D(\mathrm{m})\) is the GGX distribution function:
            $$D(\mathrm{m}) = \frac{\alpha^2 \chi^+ (\mathrm{m} \cdot \mathrm{n})}{\pi \cos^4 \theta_m (\alpha^2 + \tan^2 \theta_m)^2}$$
            where \(\chi^+\) is the positive characteristic function (\(\chi^+(a) = 1\) if \(a> 0\) and \(\chi^+(a) = 0\) if \(a \leq 0\)), and \(\theta_m\) is the angle between \(\mathrm{m}\) and \(\mathrm{n}\),
          </li>
          <li>\(G(\mathrm{i}, \mathrm{o}, \mathrm{m})\) is the shadowing-masking function of the GGX distribution:
            $$G(\mathrm{i}, \mathrm{o}, \mathrm{m}) = G_1(\mathrm{i},\mathrm{m}) G_1(\mathrm{o}, \mathrm{m})$$
            and
            $$G_1(\mathrm{v},\mathrm{m}) = \chi^+((\mathrm{v} \cdot \mathrm{m}) (\mathrm{v} \cdot \mathrm{n})) \frac{2}{1 + \sqrt{1 + \alpha^2 \tan^2 \theta_v}}$$
            where \(\theta_v\) is the angle between \(\mathrm{v}\) and \(\mathrm{n}\).
          </li>
          <li>\(I\) is the "power" of the light source.  See more details on how the power is computed in <a href="#implementation-details">the implementation details section.</a></li>
        </ul>
      </p>

      <p>When you've completed this section, the "globe" scene should look like this:</p>

      <table class="table table-bordered">
        <tr>
          <td align="center"><a href="images/a2/globe-forward-nonormalmap.png"><img src="images/a2/globe-forward-nonormalmap.png" width="300"/></a></td>
          <td align="center"><a href="images/a2/globe-forward-nonormalmap-closeup.png"><img src="images/a2/globe-forward-nonormalmap-closeup.png" width="300"/></a></td>
        </tr>
      </table>

      <h3>Programming Task 2: Anisotropic Microfacet Shading Model</h2>

      <p>For an anisotropic surface, the NDF is no longer a function only of the angle between the normal and the half vector, but depends on the components of the half vector in the \(\mathrm{t}\) and \(\mathrm{b}\) directions independently, where \(\mathrm{t}\) and \(\mathrm{b}\) are the tangent and bitangent vectors in the orthogonal frames supplied by the <tt>TriangleMesh</tt> class.  The model we'll use is almost the same as the isotropic one.  The only differences are in \(D\) and \(G_1\), which must now be calculated using the tangent space and two roughness parameters \(\alpha_X\) and \(\alpha_Y\) specificed separately to indicate the width of the NDF in the direction of \(\mathrm{t}\) and \(\mathrm{b}\) respectively.</p>  

      <p>Let us first discuss the computation of the tangent space.  The vertex shader will pass three attribute variables, <tt>geom_normal</tt>, <tt>geom_tangent</tt>, and <tt>geom_bitangent</tt>, to the fragment shader.  The vectors contained in these variables are interpolated from those same vectors at the vertices.  Hence, they are not necessarily orthonormal or even normalized.  To recover an orthonomal frame (\(\mathbf{t}\), \(\mathbf{b}\), \(\mathbf{n}\)), we suggest that you:
      <ol>
        <li>Normalize the <tt>geom_normal</tt> variable to get the normal vector \(\mathbf{n}\).</li>
        <li>Project <tt>geom_tangent</tt> to the plane perpendicular to \(\mathbf{n}\) and then normalize it to get \(\mathbf{t}\).</li>
        <li>Compute the cross product \(\tilde{\mathbf{b}} = \mathbf{n} \times \mathbf{t}\).</li>
        <li>If the dot product between <tt>geom_bitangent</tt> and \(\tilde{\mathbf{b}}\) is greater than 0, set \(\mathbf{b} = \tilde{\mathbf{b}}\).  Otherwise, set \(\mathbf{b} = -\tilde{\mathbf{b}}\).</li>
      </ol>
      In other words, we want a frame where the normal is parallel to the interpolated value.  The tangent vector is as close as possible to the interpolated value but perpendicular to the normal.  The bitangent is "redefined" to be parallel to the cross product between \(\mathbf{n}\) and \(\mathbf{t}\), but pointing in the direction that preserves the handedness given by the original mesh data.</p>

      <p>Consider the coordinate frame with the tangent \(\mathrm{t}\) as its x-axis, the bitangent \(\mathrm{b}\) as its y-axis, and the surface normal \(\mathrm{n}\) as its z-axis.  Let \(m_t\), \(m_b\), and \(m_n\) be the scalars such that 
      $$\mathrm{m} = m_t \mathrm{t} + m_b \mathrm{b} + m_n \mathrm{n}.$$ (Note that \(m_n = \mathrm{m} \cdot \mathrm{n} = \cos \theta_m\), \(m_t = \mathrm{m} \cdot \mathrm{t}\), and \(m_b = \mathrm{m} \cdot \mathrm{b}\).) Then, the anisotropic version of the GGX distribution is given by:
      $$D(\mathrm{m}) = \frac{\chi^+(\mathrm{m} \cdot \mathrm{n})}{ \pi \alpha_X \alpha_Y \bigg(m_n^2 + \frac{m_t^2}{\alpha_X^2} + \frac{m_b^2}{\alpha_Y^2} \bigg)^2}.$$
      (See if you can show that the above expression is the same as the definition of \(D\) in Task 1 when \(\alpha_X = \alpha_Y\).)
      The shadowing-masking function \(G\) is the same as that of the isotropic function, except that the roughness parameter \(\alpha\) must be calculated from \(\alpha_X\) and \(\alpha_Y\) as follows:
      $$\alpha = \sqrt{\frac{\alpha_X^2 m_t^2 + \alpha_Y^2 m_b^2}{m_t^2 + m_b^2} }.$$
      </p>      

      <p>Edit <tt>resources/Shading/shaders/forward/anisotropic_microfacet.frag</tt> to put the above formulas into action.  After completing this exercise, the rendering of the "vinyl" scene should look like the images below.
      </p>

      <table class="table table-bordered">
        <tr>
          <td align="center"><a href="images/a2/vinyl-forward.png"><img src="images/a2/vinyl-forward.png" width="300"/></a></td>
          <td align="center"><a href="images/a2/vinyl-forward-topview.png"><img src="images/a2/vinyl-forward-topview.png" width="300"/></a></td>
          <td align="center"><a href="images/a2/vinyl-forward-grazingview.png"><img src="images/a2/vinyl-forward-grazingview.png" width="300"/></a></td>
        </tr>
      </table>

      <h3>Programming Task 3: Tangent-Space Normal Mapping</h3>

      <p>Edit:
        <ul>
          <li><tt>resources/Shading/shaders/forward/isotropic_microfacet.frag</tt></li>
          <li><tt>resources/Shading/shaders/forward/anisotropic_microfacet.frag</tt></li>
        </ul>
        so that the forward renderer implements a tangent space normal map. The only difference between this model and the standard version is how the normal at the shaded point is computed.  In the standard version, we just normalize the vector given to us through the attribute variable <tt>geom_normal.</tt>  In this version, however, we first need to compute the orthonormal tangent space at the shaded point and then use it, together with the texture value of the normal map, to compute the shading normal. For the anisotropic case, we will then need to compute the orthonormal shading frame using this extracted normal.
      </p>

      <p>The normal map encodes the shading normal as a color as follows:
        $$\begin{bmatrix} r \\ g \\ b\end{bmatrix} = \begin{bmatrix} (\bar{\mathbf{n}}_x + 1) /2 \\ (\bar{\mathbf{n}}_y + 1) /2 \\ (\bar{\mathbf{n}}_z + 1) /2 \end{bmatrix}$$
        where \(\bar{\mathbf{n}} = (\bar{\mathbf{n}}_x, \bar{\mathbf{n}}_y, \bar{\mathbf{n}}_z)^T\), in tangent space, being encoded.  You should recover the tangent space normal from the color of the texture at the shaded point.  Then, the shading normal \(\mathbf{n}_{\mathrm{shading}}\) is given by:
        $$ \mathbf{n}_{\mathrm{shading}} = \text{normalize}(s\bar{\mathbf{n}}_x \mathbf{t} + s\bar{\mathbf{n}}_y \mathbf{b} + \bar{\mathbf{n}}_z \mathbf{n})$$ where \(s\) is a scalar given by the uniform <tt>mat_normalTextureScale</tt>. Proceed by using the shading normal to shade the fragment, computing the shading tangent and shading bitangent as necessary using the steps above.
      </p>

      <p>After implementing the shader, the "globe" scene should become much more interesting:</p>

      <table class="table table-bordered">
        <tr>
          <td align="center"><a href="images/a2/globe-forward.png"><img src="images/a2/globe-forward.png" width="300"/></a></td>
          <td align="center"><a href="images/a2/globe-forward-closeup.png"><img src="images/a2/globe-forward-closeup.png" width="300"/></a></td>
        </tr>
      </table>

      <h3>Programming Task 4: Deferred Shading</h3>

      <p>Now that everything works in forward shading mode, you're ready to implement the deferred shading pipeline.  In the shader code this mainly involves moving the same fragment shader code around to split it into a first-pass shader for each material and a second-pass ubershader; stubs for this have been provided in <tt>resources/Shading/shaders/deferred/</tt> directory.  The first-pass shaders have the same inputs as the corresponding forward-pass shaders and write the intermediate data into some G-buffers.  The second-pass ubershader calls the same code that is used in forward rendering to compute the shaded fragment colors.</p>

      <p>The larger difference is on the C++ side.  You will need to make several additions:
        <ul>
            <li>At initialization create an FBO that owns enough color textures to hold the required intermediate data.  Our solution uses four RGBA buffers.</li>
            <li>Create a separate branch in <tt>ShadingApp::drawContents</tt> that runs when <tt>mode == Mode::Deferred</tt> that first binds the G-buffer FBO, renders the scene with the first-pass shader programs, then binds the buffer that will be input for the sRGB shader and renders a full-screen quad with the ubershader program, providing the G-buffers to the shader programs as textures.</li>
            <li>Retain the third pass that converts to sRGB and writes to the default framebuffer.
        </ul>
      You may find it helpful to write dummy second-pass shaders that copy the G-buffers straight to the output, so that you can see what data is being stored. You can set the uniform <tt>convertToSRGB</tt> in <tt>srgb.frag</tt> to false to turn off gamma correction without changing the pipeline in this case. When everything works, you should find that switching between forward and deferred modes has no visible effect on the image.

      <h3>Programming Task 5: Bloom Effect</h3>

      <p>Edit:
        <ul>
          <li><tt>Bloom/BloomApp.cpp</tt></li>
          <li><tt>resources/Bloom/shaders/bloom.frag</tt></li>
        </ul>
        so that it renders the bloom effect when enabled.
      </p>

      <p>The bloom is a visual effect that aims to simulate the phenomenon in which imperfections in the optics of the eye (or of camera lenses) produce halos of light around bright objects.  A nice model for this effect is described by <a href="http://dx.doi.org/10.1145/218380.218466">Spencer et al.</a>; essentially it causes the image you see to be convolved with a filter that is very sharp at the center but has long, very faint tails.</p>
      <table align="center">
        <tr><td align="center"><img src="images/a2/glare-model.png" width="400"></td></tr>
      </table>
      <p>When filtering most parts of the image, it will have essentially no effect, since the tails of the filter are so faint, but when something like the sun comes into the frame, the pixel values are so high that the faint tails of the filter contribute significantly to other parts of the image.</p>

      <p>The problem is, this filter is too big to work with directly: the tails should extend a large fraction of the size of the image.  And worse, it is not separable.  So doing a straight-up space-domain convolution is hopeless.</p>

      <p>Instead, we approximate this filter with the sum of an impulse and several Gaussian filters.  The result is that the filter we will use is \[0.8843 \delta(x) + 0.1 g(6.2,x) + 0.012 g(24.9,x) + 0.0027 g(81.0,x) + 0.001 g(263,x)\] where \(g(\sigma, x)\) is a normalized Gaussian with standard deviation \(\sigma\). </p>

      <table align="center">
        <tr><td align="center"><img src="images/a2/glare-fit.png" width="400"></td></tr>
      </table>

      <p> You'll find these weights and the standard deviations for the kernels in the <tt>bloomFilterScales</tt> and the <tt>bloomFilterStdev</tt> members, respectively.  There is a parameter <tt>bloomAmpFactor</tt> that is controlled by the slider that increases the weights of the more blurred kernels. This amplification is calculated and stored in the <tt>weights</tt> variable in the <tt>drawContents()</tt> method.</p>

      <p>Then, we convolve the rendered images with 4 Gaussian kernels, each with different width, to blur it.  The results of the convolutions are as follows:        
      </p>

      <table class="table table-bordered">
        <tr>
          <td align="center"><a href="images/a2/sunset-blur-1.png"><img src="images/a2/sunset-blur-1.png" width="150"/></a></td>
          <td align="center"><a href="images/a2/sunset-blur-2.png"><img src="images/a2/sunset-blur-2.png" width="150"/></a></td>
          <td align="center"><a href="images/a2/sunset-blur-3.png"><img src="images/a2/sunset-blur-3.png" width="150"/></a></td>
          <td align="center"><a href="images/a2/sunset-blur-4.png"><img src="images/a2/sunset-blur-4.png" width="150"/></a></td>
        </tr>
        <tr>
          <td align="center">Blur #1</td>
          <td align="center">Blur #2</td>
          <td align="center">Blur #3</td>
          <td align="center">Blur #4</td>
        </tr>
      </table>

      <p>We scale each image by the constants stored in <tt>bloomFilterScales</tt> array and add the scaled images to the original image to produce the final image.</p>

      <table align="center">
        <tr>
          <td align="center" valign="center">\(k_0\)<a href="images/a2/sunset-no-bloom.png"><img src="images/a2/sunset-no-bloom.png" / width="100"></a></td>
          <td align="center" valign="center">\(+\)</td>
          <td align="center" valign="center">\(k_1\)<a href="images/a2/sunset-blur-1.png"><img src="images/a2/sunset-blur-1.png" / width="100"></a></td>
          <td align="center" valign="center">\(+\)</td>
          <td align="center" valign="center">\(k_2\)<a href="images/a2/sunset-blur-2.png"><img src="images/a2/sunset-blur-2.png" / width="100"></a></td>
          <td align="center" valign="center">\(+\)</td>
          <td align="center" valign="center">\(k_3\)<a href="images/a2/sunset-blur-3.png"><img src="images/a2/sunset-blur-3.png" / width="100"></a></td>
          <td align="center" valign="center">\(+\)</td>
          <td align="center" valign="center">\(k_4\)<a href="images/a2/sunset-blur-4.png"><img src="images/a2/sunset-blur-4.png" / width="100"></a></td>
          <td align="center" valign="center">&nbsp;&nbsp;\(=\)&nbsp;&nbsp;</td>
          <td align="center" valign="center"><a href="images/a2/sunset-mipmap-bloom.png"><img src="images/a2/sunset-mipmap-bloom.png" / width="100"></a></td>
        </tr>
        <tr>
          <td align="center">Original</td>
          <td></td>
          <td align="center">Blur #1</td>
          <td></td>
          <td align="center">Blur #2</td>
          <td></td>
          <td align="center">Blur #3</td>
          <td></td>
          <td align="center">Blur #4</td>
          <td></td>
          <td align="center">Final</td>
        </tr>
      </table>

      <p>The images below show the differences between the original and the final image with the bloom effect fully applied:</p>

      <table class="table table-bordered">
        <tr>
          <td align="center"><a href="images/a2/sunset-no-bloom.png"><img src="images/a2/sunset-no-bloom.png" / width="200"></a></td>
          <td align="center"><a href="images/a2/sunset-mipmap-bloom.png"><img src="images/a2/sunset-mipmap-bloom.png" / width="200"></a></td>
          <td align="center"><a href="images/a2/sunset-mipmap-bloom-amp.png"><img src="images/a2/sunset-mipmap-bloom-amp.png" / width="200"></a></td>
        </tr>
        <tr>
          <td align="center"><a href="images/a2/eclipse-no-bloom.png"><img src="images/a2/eclipse-no-bloom.png" / width="200"></a></td>
          <td align="center"><a href="images/a2/eclipse-mipmap-bloom.png"><img src="images/a2/eclipse-mipmap-bloom.png" / width="200"></a></td>
          <td align="center"><a href="images/a2/eclipse-mipmap-bloom-amp.png"><img src="images/a2/eclipse-mipmap-bloom-amp.png" / width="200"></a></td>
        </tr>
        <tr>
          <td align="center">Original image</td>
          <td align="center">Final image</td>
          <td align="center">Amped up bloom</td>
        </tr>
      </table>

      <p>We have included an implementation of this effect, which you can view by changing the mode to "Brute Force Bloom". It uses a temporary buffer to hold each blur result and then adds them into the original image as another pass.  It uses filter sizes that are 3 times the standard deviation. Because Gaussian filters are separable, it performs each blurring operation in two passes, once for each image dimension.</p>

      <p>The problem with this approach is that it is really slow. Depending on your setup, it may grind your program to a halt, or your graphics card might just give up. The issue is that the large blur operations need to access too many pixels of the source image in each pass. A good way to speed up large blurs is to shrink the image, blur it, and then enlarge it back to size.  If you resample the image so that is is smaller by a factor \(\alpha\), then apply a Gaussian of width \(\alpha\sigma\), then resample back to the original size, the result will be quite a good approximation of blurring the full image by a Gaussian of width \(\sigma\), as long as \(\alpha\sigma\) does not get too small.  (We recommend keeping this effective standard deviation above 4 pixels.)</p>

      <p>We recommend doing this by taking advantage of the machinery already available to you as part of the graphics pipeline. In particular, mipmaps are an effective way of shrinking an image down to scales that are appropriate to blur efficiently; furthermore, you can use built-in texture sampling machinery to resample blurred images to return them to their original size. The process for this is explained below:</p>
      <ul>
        <li>Take the target texture of the sun scene pass and generate a mipmap of it (look in <tt>GLWrap::GLTexture2D</tt> for a helpful function here.)</li>
        <li>Calculate the levels of the mipmap which require blurring; these should be based on the Gaussian standard deviations and the maximum blur window.</li>
        <li>For each of the levels calculated above, perform a Gaussian blur at the appropriate scale. You can use the provided <tt>gaussianBlurPass()</tt> method to do this.
        You can perform each pass into a mipmap associated with a single texture as long as the target texture has had mipmaps allocated appropriately.</li>
        <li>Perform a pass that takes the blurred images along with the original image and combines them into a single image using the weights described above. If the blurred images are stored at different levels of a mipmap, you need to <a href="https://www.khronos.org/opengl/wiki/Sampler_Object">set the texture's minification filter</a> to <tt>GL_LINEAR_MIPMAP_NEAREST</tt> to be able to access the appropriate mipmap levels. The GLSL function <a href="https://www.khronos.org/opengl/wiki/Sampler_(GLSL)#Lod_texture_access"><tt>textureLod</tt></a> can sample from a given mipmap level.</li>
        <li>Perform a final pass to do sRGB conversion when rendering to the screen.</li>
      </ul>


      <h3>Framework</h3>

      <p>The code for this assignment is included in <a href="https://github.coecis.cornell.edu/cs5625/Spring19Framework">the class repository</a> as a new commit. We've included a couple new libraries for this part, so please run <tt>git submodule update --recursive</tt> after pulling to download the necessary additional components. Remember to rerun CMake as well; you may need to remove your build directory and create a fresh one to stop cached settings from carrying over from the previous assignment. This will create <tt>Shading</tt> and <tt>Bloom</tt> compilation targets.</p>

      <p>We've added a number of classes for dealing with 3D scenes which are found in the <tt>Common</tt> directory. You shouldn't need to modify any of these, but if you're curious how a scene is represented, that is the place to look. In particular, objects in a scene are referenced in a scene graph, where each node in a tree has a transformation relative to its parent. Scenes are read in using a library called the Open Asset Import Library, or <a href="http://www.assimp.org/">Assimp</a>. Our scenes are specified in COLLADA format, but Assimp can be configured to read in many different file formats. Materials are specified separately in an XML file; see the comments <tt>Common/Material.hpp</tt> for more information. Please be aware that these scene files may change in a future assignment.</p>

      <p>The Shading target program includes an interactive camera can be controlled using the following mouse actions:
        <ul>
          <li>LMB: Orbiting the camera.</li>
          <li>RMB: Translating the camera view point.</li>
          <li>Scroll: Changing the camera zoom.</li>
        </ul>
      </p>    

      <h3 id="what-to-submit">What to Submit</h3>

      <p>You should submit a ZIP file containing all the source codes and data in the <tt>Shading</tt>, <tt>Bloom</tt>, and <tt>resources</tt> directories.  All the code you have written should be well commented and easy to read, and header comments for all modified files should appropriately indicate authorship.  Be sure to cite sources for any code or formulas that came from anywhere other than your head or this assignment document.  Also, put in the directory a README file explaining any implementation choices you made or difficulties you encountered.</p>

      <h3 id="implementation-details">Implementation Details</h3>

      <h4>Use of Textures</h4>

      <p>Some material properties are stored as a scalar/vector value and a texture.  For example, in the <tt>IsotropicMicrofacet</tt> class, there is both the <tt>alpha</tt> field and the <tt>alphaTexture</tt> field.  In such a case, the scalar/vector value must be defined, but the texture can be left unspecified (i.e., null).</p>

      <p>In the corresponding fragment shader, there will be 3 uniforms related to the material parameter.  One uniform corresponds to the scalar/vector value (for example, <tt>mat_alpha</tt>).  One uniform serves as a flag telling whether there exists the corresponding texture (for example, <tt>mat_hasAlphaTexture</tt>).  One uniform corresponds to the texture (for example, <tt>mat_alphaTexture</tt>).  If the texture is undefined, you should use the scalar/vector value to calculate shading.  If the texture is undefined, you should fetch the texture value, multiply it with the scalar/vector value, and use the product to compute shading.  For example, we might compute the alpha value that will used later for shading in the fragment shader as follows:</p>

      <pre>
float alpha = mat_alpha;
if (mat_hasAlphaTexture) {
  alpha *= texture(mat_alphaTexture, geom_texCoord).x;
}</pre>

      <h4>Point Light Sources</h4>

      <p>Light sources in this assignment are all point light sources and implemented in the <tt>PointLight</tt> class in <tt>Common/Scene.hpp</tt>.  A point light is specified by three parameters: its position, its color (i.e., power), and its 3 attenuation coefficients.  The power reaching a shaded point is computed in a non-physical manner as follows:
        $$ \mathrm{power} = \frac{color}{A + Bd + Cd^2} $$
      </p>
      where \(d\) is the distance between the light source and the shaded point, \(A\) is the constant attenuation coefficient, \(B\) is the linear attenuation coefficient, and \(C\) is the quadratic attenuation coefficient.


